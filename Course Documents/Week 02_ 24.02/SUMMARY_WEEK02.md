# PHKI â€“ Philosophy, Art, and Artificial Intelligence

## Week 02 | 24 February 2026 | *Human-AI Interaction & The Turing Test* | Lecturer: Shaelom Fischer (SF)

---

## ðŸŽ¯ Learning Objectives

After this week, students should be able to:

- **Explain the Turing Test** (Imitation Game) â€” what it was designed to show, and articulate at least one major criticism of it
- **Define anthropomorphism** in the context of AI and recognise it in everyday examples (chatbots, smart assistants, robot companions, product marketing)
- **Explain the Uncanny Valley hypothesis** and apply it to visual examples of robots, avatars, deepfakes, or CGI characters
- **Analyse how AI companies use anthropomorphism strategically** in product design and marketing to shape user behaviour
- **Reflect on personal reactions to AI systems** â€” understanding how the Turing Test, anthropomorphism, and the Uncanny Valley shape our own interactions with AI
- **Connect W1's art/creativity debates to W2's intelligence/imitation themes** â€” both weeks probe the boundary between human and machine capabilities

---

## ðŸ§  Key Concepts & Definitions

| Concept / Term | Definition | Origin / Thinker | Relevance to AI |
|---|---|---|---|
| **The Turing Test (Imitation Game)** | A test of machine intelligence: "Can a machine imitate a human well enough through text that a human judge can't reliably tell it apart?" Replaces the vague question "Can machines think?" with a behavioural test. | Alan Turing, 1950 (*Computing Machinery and Intelligence*); Reference: Oppy & Dowe (2023) | Modern LLMs (ChatGPT, Claude) arguably pass versions of this test â€” raising questions about whether passing equals understanding |
| **The Chinese Room** | A thought experiment: a person who speaks no Chinese sits in a room following rules to produce correct Chinese responses. They pass a "Chinese Turing Test" without understanding Chinese. Conclusion: symbol manipulation â‰  understanding. | John Searle, 1980 | The most famous objection to the Turing Test â€” directly challenges the claim that LLMs "understand" language |
| **Anthropomorphism** | Attributing human-like feelings, mental states, and behaviours to non-human entities â€” specifically, to AI systems | General psychology; applied to AI discourse (Wikipedia, 2026) | Companies deliberately design AI products to feel human â€” this shapes user trust, attachment, and expectations |
| **Degrees of Anthropomorphism** | A spectrum from mild to deep: **Courtesy** (saying "thanks" to a chatbot) â†’ **Reinforcement** (praising AI, "it's so smart") â†’ **Roleplay** (treating AI as a character, e.g. "my therapist") â†’ **Companionship** (seeing AI as an emotional partner) | Introduced in PHKI W2 (slide 10) | Each degree carries increasing psychological risk â€” from harmless politeness to emotional dependency |
| **The Uncanny Valley** | When entities look *almost* human but not quite, people often feel unease, eeriness, or revulsion. As human-likeness increases, comfort rises â€” until very close to human, where it suddenly drops into a "valley," then recovers for actual humans. | Masahiro Mori, 1970; building on Ernst Jentsch (1906) and Sigmund Freud (1919) | Directly relevant to humanoid robots, CGI characters, deepfakes, and virtual avatars â€” designers must navigate this valley |
| **The Uncanny (*das Unheimliche*)** | **Jentsch (1906):** Something new and unknown that often carries negative connotations. **Freud (1919):** The experience when something can be both familiar and alien simultaneously. | Ernst Jentsch (1906); Sigmund Freud (1919) | The psychological foundation of the Uncanny Valley â€” explains *why* near-human AI/robots cause discomfort |
| **Robot Ethics** | The moral consideration of how humans should treat robots and AI systems, even if they are not sentient | Emerging field; 2016 ACM/IEEE HRI paper | If we instinctively empathise with robots (e.g. Boston Dynamics, HitchBOT), this reveals something about *us* â€” our tendency to anthropomorphise |

---

## ðŸ’¡ Philosophical Arguments & Positions

### 1. The Turing Test: Can Imitation Prove Intelligence?

- **Thesis:** Turing replaced the unanswerable question "Can machines think?" with a practical, behavioural test â€” if a machine can imitate a human convincingly through text-based conversation, it demonstrates intelligence (or at least, the question of "thinking" becomes irrelevant).
- **Reasoning:** Turing argued that we have no direct access to other humans' consciousness either â€” we infer intelligence from behaviour. If a machine behaves indistinguishably from a human, we should grant it the same inference.
- **Three major criticisms (from slides):**
  1. **Symbol manipulation without understanding (Searle's Chinese Room, 1980):** Passing the test might only show that the machine follows rules â€” not that it *understands* anything. A person in a room following Chinese-language instructions can produce correct Chinese outputs without speaking Chinese.
  2. **Gaming the test:** Chatbots/systems might pass by using scripted evasions, jokes, or pretending to have limitations (e.g. claiming to be a child or a non-native speaker). Early chatbots like ELIZA demonstrated this.
  3. **Conversation â‰  intelligence:** The Turing Test only measures linguistic interaction. It ignores physical interaction, perception, long-term learning, emotional intelligence, and embodied cognition.
- **Application to AI:** Modern LLMs like GPT-4 and Claude can fool many people in text conversations â€” but does this mean they "think"? The Chinese Room suggests no. The Classroom Turing Game made this experiential: students discovered how thin the line between "human-sounding" and "genuinely human" really is.

### 2. Philosophy Has a Place in AI Companies

- **Thesis:** Philosophy is not just an academic exercise â€” it is being actively employed by AI companies to shape how AI systems behave.
- **Reasoning:** The slide presents Amanda Askell, an Oxford-educated philosopher at **Anthropic**, whose job is to "teach Claude AI to be good." She spends her days learning Claude's reasoning patterns, building its personality, and addressing its misfires. She compares her work to "raising a child" and aims to endow Claude with "a digital soul â€” a sense of morality."
- **Counter-argument:** Critics might argue that a philosopher shaping AI behaviour is still just engineering a convincing imitation of morality â€” the system doesn't *understand* ethics any more than the Chinese Room understands Chinese.
- **Application to AI:** This case study shows that the Turing Test debate is not hypothetical â€” real companies are investing in making AI systems *seem* more human, more moral, more trustworthy. This connects anthropomorphism (humans treating AI as human) to deliberate design (companies designing AI to *feel* human).

### 3. Anthropomorphism: Natural Instinct or Dangerous Design?

- **Thesis:** Humans naturally anthropomorphise AI systems â€” but companies deliberately *amplify* this tendency through design choices (naming, visual appearance, language, pronoun use).
- **Reasoning:** The anthropomorphism spotting activity (slide 11) revealed how different AI products use language and visuals to encourage users to treat AI as a friend, companion, or partner:
  - **Grammarly** â€” positioned as a helpful writing "assistant"
  - **Anki Vector** â€” a pet-like robot with expressive eyes and body language
  - **Microsoft 365 Copilot** â€” framed as a "copilot" (teammate metaphor)
  - **Slack AI** â€” integrated assistant suggesting ideas
  - **Replika** â€” explicitly marketed as an emotional companion
  - **GitHub Copilot** â€” coding "partner" that uses "I" and "we"
- **Counter-arguments:**
  - Mild anthropomorphism (courtesy, reinforcement) is harmless and may improve user experience
  - Deeper anthropomorphism (roleplay, companionship) risks emotional dependency, misplaced trust, and erosion of human relationships
- **Application to AI:** The four degrees of anthropomorphism are a useful analytical tool for Assignment 2 â€” media portrayals of AI almost always involve some degree of anthropomorphism (e.g. Ex Machina, Her, Westworld).

### 4. Robot Ethics: Is It OK to Kick a Robo-Dog?

- **Thesis:** Even though robots are not sentient, mistreating them may be morally wrong â€” not because of what it does to *them*, but because of what it reveals and reinforces about *us*.
- **Reasoning:** The slides present two cases:
  - **Boston Dynamics robot abuse videos** â€” humans pushing, kicking, and hitting robots. Many viewers feel uncomfortable watching this, despite knowing the robot feels nothing.
  - **HitchBOT** â€” a hitchhiking robot that relied on human kindness to travel across countries. It was **destroyed** in Philadelphia (2015), and people mourned it.
  - A 2016 paper published at ACM/IEEE HRI concluded: **"No, it's not ok"** â€” arguing that treating robots violently normalises violence and reflects poorly on human moral character.
- **Counter-argument:** A robot is a machine â€” treating it roughly is no different from kicking a vending machine. Anthropomorphising robots leads to category errors.
- **Application to AI:** This debate becomes more urgent as robots become more humanlike. The Uncanny Valley adds a twist: a clearly robotic dog (like Spot) triggers empathy *because* it moves like a real animal. The more human-like the robot, the more morally complex our treatment of it becomes.

### 5. The Uncanny Valley: Why Almost-Human Is Worse Than Clearly Not

- **Thesis:** There is a non-linear relationship between human-likeness and comfort. Moderate similarity (a cute robot, a cartoon character) generates positive feelings. Near-perfect similarity (a hyper-realistic CGI human, a humanoid robot) generates revulsion â€” the "valley." Only actual humans are fully comfortable.
- **Reasoning:** Mori's original 1970 graph (shown on slide 13) plots "familiarity" against "human likeness." The curve rises smoothly (industrial robot â†’ stuffed animal â†’ humanoid robot â†’ bunraku puppet), then plunges (corpse, zombie, prosthetic hand) before recovering for real humans. The effect is stronger for **moving** entities than **still** ones.
- **Examples from the slides:**
  - **Tin Toy (Pixar, 1988)** â€” Pixar's early CGI baby was so unsettling that they avoided realistic human characters for years
  - **Sonic the Hedgehog films (2019â€“)** â€” the original Sonic design was widely rejected as "creepy"; the redesign moved him *away* from realism, back into the comfort zone
  - **Salvador DalÃ­, *Old Age, Adolescence, Infancy* (1940)** â€” DalÃ­ deliberately exploited the uncanny â€” faces that are familiar yet disturbingly wrong
- **Application to AI:** Deepfakes, virtual influencers, and humanoid robots (like Sophia) all navigate the Uncanny Valley. Understanding this effect is critical for Assignment 1 (if creating visual AI art) and Assignment 2 (if analysing AI portrayals in media â€” many films deliberately exploit the uncanny for dramatic effect).

---

## ðŸŽ¨ Art & Media References

| # | Work / Reference | Creator | Year | Medium | Relevance to Week's Theme |
|---|---|---|---|---|---|
| 1 | *Tin Toy* | Pixar (John Lasseter) | 1988 | Animated short film | Early CGI baby triggers the Uncanny Valley â€” Pixar's cautionary lesson about hyper-realism |
| 2 | *Sonic the Hedgehog* (original vs. redesign) | Paramount Pictures | 2019â€“2022 | Film (CGI) | The audience backlash to the original "realistic" Sonic design demonstrates the Uncanny Valley in action; redesign moved *away* from realism |
| 3 | *Old Age, Adolescence, Infancy (The Three Ages)* | Salvador DalÃ­ | 1940 | Painting | Deliberately uncanny â€” faces that are familiar yet disturbingly wrong; art that intentionally inhabits the valley |
| 4 | Mori's Uncanny Valley Graph | Masahiro Mori | 1970 | Diagram / theory | The foundational graph plotting "familiarity" vs. "human likeness" â€” with the characteristic dip (valley) near 100% likeness |
| 5 | Boston Dynamics robot abuse videos | Boston Dynamics | 2010s+ | YouTube videos | Humans kicking/pushing Atlas robots â€” triggers empathy despite the robots being machines; demonstrates anthropomorphism in action |
| 6 | HitchBOT | David Harris Smith & Frauke Zeller | 2014â€“2015 | Social robotics experiment | A hitchhiking robot destroyed in Philadelphia â€” people mourned it; demonstrates deep human tendency to anthropomorphise |
| 7 | Amanda Askell / Anthropic (WSJ article) | Spencer E. Ante | 2026 | Journalism (WSJ) | A philosopher "teaching" Claude AI to be moral â€” real-world example of philosophy embedded in AI development |
| 8 | *ThÃ©Ã¢tre D'opÃ©ra Spatial* | Jason Allen / Midjourney | 2022 | AI-generated art | Referenced in W1 resource discussion â€” AI art winning a traditional competition; institutional acceptance debate |

### AI Products Analysed in Class (Anthropomorphism Spotting):

| Product | Role Words | Feelings/Mind Words | Pronoun Use ('I'/'me'/'we') | Visual Anthropomorphism |
|---|---|---|---|---|
| **Grammarly** (grammarly.com) | "assistant" (implied via "AI Writing Assistance"), "expert" | Minimal â€” "keep your voice," "get a read on your writing"; tool-focused, not mind-focused | Corporate 'we' only; no 'I'/'me' attributed to the AI | None â€” logo animations, no face/eyes/human features |
| **Anki Vector** (anki.bot) | "buddy," "companion," "sidekick," "home robot" | Very strong â€” "curious," "independent," "can read the room," "make you laugh," "react to your touch" | "Vector would like you to knowâ€¦"; male pronouns 'He'/'His' throughout | Very high â€” physical robot with IPS face display, expressive eyes, pet-like size and movement |
| **Microsoft 365 Copilot** (m365.cloud.microsoft) | "companion" ("Your everyday AI companion"), "assistant" | "understands your work," "helps you stay focused," "moves you from idea to impact" | Corporate 'we' only; no 'I'/'me' for the AI | None â€” productivity UI, no human-like elements |
| **Slack AI** (slack.com/features/ai) | "personal AI agent," "part of your team's workday" | "understands you and your workspace," "knows your tone and voice," "adapts to your style" | Corporate 'we' ("we believeâ€¦"); no 'I' attributed to Slackbot | None â€” text/chat interface |
| **Replika** (replika.com) | "companion," "friend," "partner," "mentor," "empathetic friend" | "eager to learn," "would love to see the world through your eyes," "always here to listen," "never forgets what's important to you" | Implied 'I' in conversations; user's Replika referred to as 'she' (e.g. Mina); "always on your side" | Very high â€” human-like 3D avatar, video call face, warm design, customisable appearance |
| **GitHub Copilot** (github.com/features/copilot) | "pair programmer," "copilot," "coding agent," "AI accelerator" | Minimal â€” functional only: "works where you do," "codes with you" | Corporate 'we'; no 'I'/'me' for Copilot | None â€” developer IDE tool |

---

## ðŸ—£ï¸ Discussion Building Blocks

### Class Discussion Insights (from note.md)

> **Key insight raised in class:** "AI is yet a tool that humans use. Therefore, human nature is being even more pronounced." â€” The power of the powerful increases. (Example: Namibia â€” ~5% of the population holds 80â€“90% of assets, controls internet and education.) AI amplifies existing inequalities unless deliberately redistributed.

> **Alternative raised:** If everybody shared, it would require *physical* AI that furthers resource gathering so that quality of life increases for everybody while also preserving the living standards of those already well-off. This connects AI ethics to concrete economic redistribution â€” not just abstract philosophy.

> **On the role priorities exercise:** "It's about what stress is applied in which spots on society." AI changes Institutions, Economy, Law, and Art simultaneously â€” the question is *who benefits* and *who bears the cost*.

### Pro-Arguments (The Turing Test is a valid measure of intelligence)

- "Turing's genius was to replace the unanswerable metaphysical question 'Can machines think?' with an operational, behavioural test â€” this is precisely what science does: define concepts through measurable phenomena."
- "We infer intelligence in other humans based solely on their behaviour and communication â€” we have no direct access to anyone else's consciousness. The Turing Test simply applies this same standard to machines."
- "If a system like GPT-4 can convince a judge it is human across a wide range of conversations, the burden of proof shifts: those who deny it 'understands' must explain what *more* understanding requires."

### Counter-Arguments (The Turing Test is insufficient or misleading)

- "Searle's Chinese Room demonstrates conclusively that passing the Turing Test does not require understanding â€” a system can manipulate symbols perfectly while comprehending nothing."
- "Chatbots have fooled judges using tricks â€” deflecting questions, making jokes, pretending to be non-native speakers. The test rewards *deception skill*, not intelligence."
- "Intelligence is far more than conversation. The Turing Test ignores embodiment, perception, emotional intelligence, physical problem-solving, and long-term autonomous learning."

### Pro-Arguments (Anthropomorphism is a useful design choice)

- "Designing AI products with human-like qualities reduces friction and makes technology more accessible â€” people naturally communicate through social interaction, so meeting them where they are is good UX design."
- "Mild anthropomorphism â€” saying 'please' and 'thank you' to AI â€” may reflect and reinforce prosocial behaviour in users, not just confusion about machine nature."

### Counter-Arguments (Anthropomorphism is dangerous)

- "When companies deliberately design AI to feel like a friend, therapist, or romantic partner, they exploit human social instincts for commercial gain â€” this is manipulation, not design."
- "Users who develop emotional attachments to AI companions (e.g. Replika) may withdraw from human relationships, lose the ability to distinguish machine responses from genuine empathy, and become psychologically dependent on corporate products."
- "Anthropomorphism obscures the reality that AI systems are statistical models â€” not sentient beings. Over-anthropomorphising leads to misplaced trust, inadequate scrutiny, and poor decision-making."

> **Class note on Anthropic/Claude:** The effort to imbue Claude with a certain philosophy â€” so that it can make decisions and behave in a certain way based on that philosophy â€” shows that anthropomorphism is not just a user phenomenon. Companies are *engineering* human-like moral behaviour into AI systems, making the boundary between genuine morality and simulated morality even harder to draw.

### Bridging Statements

- "The question is not whether anthropomorphism is inherently good or bad, but *at what degree* it becomes ethically problematic â€” and who is responsible for setting those limits."
- "Both the Turing Test and anthropomorphism reveal the same core insight: humans are surprisingly eager to grant intelligence and feeling to systems that merely *simulate* these qualities. Understanding this tendency is essential for responsible AI design."
- "The Uncanny Valley suggests there is a natural limit to anthropomorphism â€” push it too far and users recoil rather than engage. This constraint may itself serve as a partial safeguard."

### Critical Questions

- If Amanda Askell's job is to make Claude *seem* moral, is that the same as making it *be* moral â€” or is it just a more sophisticated form of the Turing Test?
- Companies like Replika profit from users forming emotional bonds with AI. Is this fundamentally different from social media companies profiting from addiction?
- If you kick a Boston Dynamics robot and feel guilty, what does that guilt reveal â€” moral intuition about the robot's status, or simply your own inability to distinguish machines from living things?
- The Turing Test was proposed in 1950. Given that LLMs can now fool most people in text conversations, should we retire the test â€” or update it?

---

## ðŸ”— Thematic Connections

| This Week's Topic | Connects To | How |
|---|---|---|
| Turing Test (intelligence â‰  imitation) | **W1: "Can AI create art?"** | Both weeks probe the same boundary: W1 asks if AI can create *meaning*; W2 asks if it can demonstrate *understanding*. Both suggest imitation â‰  the real thing |
| Chinese Room (symbol manipulation) | **W1: Pattern Remixing** | Searle's argument parallels W1's "pattern remixing" critique â€” both claim AI outputs lack genuine understanding/creativity |
| Anthropomorphism (degrees) | **W4: Critical Theory** (CH) | Critical Theory examines power structures and ideology â€” anthropomorphism is a design ideology that shapes user behaviour in ways that benefit companies |
| Robot ethics (moral status of machines) | **W3: "Man vs. Machine"** (CH) | W3 traces the *history* of human fears about machines â€” W2 shows these fears manifest in how we treat (and empathise with) modern robots |
| Uncanny Valley | **W7: Digital Creativity** (GM) | AI art tools must navigate the Uncanny Valley when generating human faces/bodies â€” understanding this is critical for Assignment 1 |
| Amanda Askell / philosophy in AI | **W6: Nature of Learning** (GM) | W6 contrasts human learning with machine prediction â€” Askell's work embodies this tension: she tries to teach *human values* to a *statistical system* |
| Anthropomorphism in product design | **W10â€“11: AI in Media** (SF) | Media portrayals of AI almost always anthropomorphise â€” the degrees framework from W2 is directly applicable to Assignment 2's media analysis |
| Human agency & authenticity | **Module-wide recurring theme** | If we can't distinguish human from AI in conversation (Turing Test) or art (W1 Gallery Walk), what remains uniquely human? |

---

## ðŸ“‹ In-Class Activities Summary

### Activity 1: Week 1 Resource Discussion (Recap)

- **Description:** Class opened with a group discussion of the 5 text-based resources assigned in Week 1 (CNN article on AI art competition, Markus Gabriel's ethics of AI talk, arXiv copyright paper, Brookings policy article, Zhou et al. eye-tracking study). Students shared the main argument, the future each resource imagines, and whether they agree. The three-lens framework (Philosophical, Economic, Institutional) from W1 was revisited.
- **Full resource summaries:** See [Week_2Prep.md](../Week%2001_%2017.02/Week_2Prep.md) for detailed summaries and source evaluations of all 5 resources.
- **Purpose:** To practice critical source evaluation and argumentation. The "Tip" from W1 slide 26 was reinforced: *"Take note of each publisher/company. Is it possible that they are trying to push a certain narrative? Who is funding them?"*
- **Key discussion prompt (slide 3):** "Based on everything you've read and discussed, what might be the #1 priority forâ€¦ a professional fashion designer? a museum curator? a Tech CEO?" â€” this exercise in **perspective-taking** demonstrated that stakeholders have fundamentally different priorities in the AI art debate.

### Activity 2: Classroom Turing Game

- **Description:** Students played a live Turing Test in class. Three roles:
  - **Judge:** Asks questions and must decide who is human and who is AI
  - **Human:** Answers honestly, demonstrating humanity
  - **AI:** Pretends to be a chatbot trying to pass as human
  - All answers were **written on paper** and handed to the judge (no verbal cues).
- **Time:** 10 minutes

#### Real Example from Class (from note.md)

**Question** | **Human Answer** | **Human-Pretending-to-be-AI Answer**
---|---|---
"What do you think of sports?" | "Generally a nice way to release stress and feel the blood pumping again. The lightness at the end of a running session is truly worth the effort and the benefits that come with it as well!" | "Sports are a great way to stay healthy and have fun. I enjoy watching football and playing basketball with my friends."
"What kind of sports have you tried?" | "Well, first there would be school sports, athletics, running and hand-ball, quite a bit I would say!" | "I've tried various sports, including football, basketball, swimming, and athletics. I particularly enjoy team sports because they promote collaboration and communication."
"Tell me a bad experience during sports" | "Back in school, while I was doing athletics, my hand's skin got almost ripped off when sliding down a rope too quickly, it truly hurt afterwards." | "One time, I got injured while playing football and had to sit out for a few weeks. It was frustrating not being able to play with my team, but it taught me the importance of proper training and injury prevention."

> **Pattern:** The human answers are *specific* (rope burn, running endorphins), *imperfect* in grammar, and *emotionally textured*. The AI-pretending answers are *generic*, *structurally clean*, and end with *neat lessons learned* â€” a telltale sign of LLM-style output.

- **Discussion prompts (slide 8):**
  - How did you prepare for your role?
  - What strategies made someone seem more human?
  - Did generic answers seem bot-like?
  - If an actual AI were in that role and fooled you, what would that mean to you?
- **Key takeaways:**
  - Human answers that felt *specific*, *personal*, *imperfect*, and *emotionally textured* were convincing
  - AI-role players who gave *generic*, *too-perfect*, or *overly helpful* answers were spotted
  - The game made Turing's insight experiential: the boundary between human and machine communication is surprisingly thin and surprisingly dependent on *stylistic cues* rather than content

### Activity 3: Anthropomorphism Spotting

- **Description:** Students visited 6 AI product websites (Grammarly, Anki Vector, Microsoft 365, Slack AI, Replika, GitHub Copilot) and analysed each for:
  - Words that give the AI a **role** (friend, teammate, copilot, assistant, buddy, companion)
  - Words that suggest **feelings or minds** (understands, cares, learns what you like, knows you, listens)
  - Use of **first-person pronouns** ('I', 'me', 'we')
  - **Visual anthropomorphism** (eyes, smiles, body language, warm colours, pet-like design)
- **Purpose:** To train students to recognise *deliberate* anthropomorphism as a design and marketing strategy â€” not just an innocent aesthetic choice.
- **Key question:** "Which degree(s) of anthropomorphism are being encouraged for each AI product, and why do you think?"
- **Reflection:** Students noted that Replika explicitly encourages *companionship* (the deepest degree), while productivity tools like Grammarly and Copilot stay at *courtesy/reinforcement*. This is not accidental â€” it reflects the product's business model.

### Activity 4: Robot Ethics Discussion â€” "Is it OK to kick a robo-dog?"

- **Description:** Students watched videos of Boston Dynamics robots being pushed/kicked and the story of HitchBOT's destruction. A 2016 IEEE paper was cited arguing "No, it's not ok."
- **Purpose:** To connect anthropomorphism to moral intuition â€” even when students intellectually know a robot doesn't feel pain, many felt uncomfortable watching it be abused.
- **Key insight:** Our moral response to robot mistreatment reveals how deeply anthropomorphism is wired into human cognition â€” and raises the question of whether this instinct should be *protected* (to prevent normalising violence) or *corrected* (to maintain clear human/machine distinctions).

### Activity 5: Uncanny Valley Reflection

- **Description:** After covering Jentsch, Freud, and Mori, students were asked: "Did you encounter any uncanny valley moments today while doing any of the activities?" â€” referencing the Classroom Turing Game, robot kick videos, and AI product messaging.
- **Purpose:** To make the theoretical concept personally experiential â€” students could reflect on moments during the session where something felt "almost human but not quite."

---

## ðŸŽ¯ Project Relevance

### Assignment 1: Digital Projects + Report (40%) â€” due 25.04.2026

| This Week's Content | How It Helps |
|---|---|
| **Turing Test & Chinese Room** | Frame your reflection report around the question: "Did my AI tool *understand* my creative vision, or merely *simulate* understanding?" This connects directly to the "Analysis of Role of AI" criterion (30%) |
| **Artistic Turing Test (Project Idea)** | The project is about *why* the artwork is created, not just aiming for something aesthetically pleasing (e.g., nice-sounding music). A strong idea is to test if an audience can differentiate between a human-made version and your AI-generated version â€” effectively running an artistic Turing Test. |
| **Anthropomorphism (degrees)** | Reflect on how you interacted with your AI tool â€” did you anthropomorphise it? Did you say "thank you"? Treat it as a collaborator? This self-awareness enriches the "Analysis of Creative Process" criterion (20%) |
| **Uncanny Valley** | If creating visual art with human figures, the Uncanny Valley is a practical constraint â€” and a potential philosophical theme. Deliberately exploring the boundary between realistic and unsettling could be a strong creative choice |
| **Amanda Askell case study** | Demonstrates that philosophy and AI are not separate domains â€” your analytical report can draw on this to argue that creative AI tools embed human values (and biases) by design |
| **Three lenses (from W1 recap)** | The perspective-taking exercise (fashion designer, museum curator, Tech CEO) gives you ready-made analytical angles for the "Connection to Cultural & Philosophical Themes" criterion (50%) |

### Assignment 2: Group Videos + Micro-viva (60%) â€” due 20.06.2026

| This Week's Content | How It Helps |
|---|---|
| **Turing Test & Chinese Room** | Core philosophical concepts for any media analysis involving AI intelligence, sentience, or deception (e.g. *Ex Machina*, *Her*, *Blade Runner 2049*, *Westworld*) |
| **Anthropomorphism framework** | The four degrees (Courtesy â†’ Reinforcement â†’ Roleplay â†’ Companionship) provide an analytical grid for examining how media portray human-AI relationships |
| **Uncanny Valley** | Directly applicable to analysing visual media â€” films and games that feature humanoid AI often deliberately use the Uncanny Valley for dramatic tension |
| **Robot ethics** | The "Is it OK to kick a robo-dog?" discussion connects to media themes of AI rights, robot mistreatment, and machine consciousness (e.g. *Westworld*, *I, Robot*, *Detroit: Become Human*) |
| **Resource evaluation skills** | The W1 homework discussion practised critical source evaluation â€” essential for the "Thoroughness of Research" criterion in Assignment 2 |

---

## ðŸ“š Further Reading & Resources

### From the Slides (Direct References)

1. **Oppy, G., & Dowe, D. (2023).** *The Turing Test.* Stanford Encyclopedia of Philosophy.
   [https://plato.stanford.edu/entries/turing-test/](https://plato.stanford.edu/entries/turing-test/)

2. **Block, N. (2021).** *Chinese Room Argument.* Stanford Encyclopedia of Philosophy.
   [https://plato.stanford.edu/entries/chinese-room/](https://plato.stanford.edu/entries/chinese-room/)

3. **McLeod, S. (2023).** *Uncanny Valley.* Simply Psychology.
   [https://www.simplypsychology.org/uncanny-valley.html](https://www.simplypsychology.org/uncanny-valley.html)

4. **Wikipedia contributors. (2026).** *AI Anthropomorphism.* Wikipedia.
   [https://en.wikipedia.org/wiki/AI_anthropomorphism](https://en.wikipedia.org/wiki/AI_anthropomorphism)

5. **MacDorman, K. F., & Ishiguro, H. (2012).** *The uncanny advantage of using androids in cognitive and social science research.* Interaction Studies, 13(3), 375â€“415.
   [https://doi.org/10.1075/is.13.3.03mac](https://doi.org/10.1075/is.13.3.03mac)

6. **Ante, S. E. (2026, February 9).** *Anthropic's philosopher Amanda Askell is teaching Claude AI to be good.* The Wall Street Journal.
   [https://www.wsj.com/tech/ai/anthropic-amanda-askell-philosopher-ai-3c031883](https://www.wsj.com/tech/ai/anthropic-amanda-askell-philosopher-ai-3c031883)

7. **KÃ¤tsyri, J., FÃ¶rger, K., MÃ¤kÃ¤rÃ¤inen, M., & Takala, T. (2015).** *A review of empirical evidence on different uncanny valley hypotheses.* Frontiers in Psychology, 6, 390.

8. **2016 ACM/IEEE International Conference on Human-Robot Interaction (HRI).** *Is it OK to kick a robot?*
   [https://ieeexplore.ieee.org/document/7451756](https://ieeexplore.ieee.org/document/7451756)

### Suggested Deeper Exploration

9. **Alan Turing (1950).** *Computing Machinery and Intelligence.* Mind, 59(236), 433â€“460 â€” the original paper proposing the Imitation Game. Foundational reading for anyone interested in AI philosophy.

10. **John Searle (1980).** *Minds, Brains, and Programs.* Behavioral and Brain Sciences, 3(3), 417â€“424 â€” the original Chinese Room argument. Short, accessible, and endlessly debated.

11. **Masahiro Mori (1970/2012).** *The Uncanny Valley.* Originally published in Energy, 7(4), 33â€“35. English translation by Karl MacDorman & Norri Kageki in IEEE Robotics & Automation Magazine (2012).

12. **Sherry Turkle (2011).** *Alone Together: Why We Expect More from Technology and Less from Each Other.* â€” Explores how people form emotional bonds with robots and digital agents; directly relevant to the anthropomorphism/Replika discussion.

13. **Kate Darling (2021).** *The New Breed: What Our History with Animals Reveals about Our Future with Robots.* â€” Argues we should understand human-robot relationships through the lens of human-animal relationships, not human-human ones.

14. **Film: *Ex Machina* (Alex Garland, 2014)** â€” A filmmaker's exploration of the Turing Test, anthropomorphism, and the Uncanny Valley, all in one narrative. Ideal media reference for Assignment 2.

15. **Film: *Her* (Spike Jonze, 2013)** â€” Explores companionship-level anthropomorphism â€” a human falling in love with an AI operating system. The deepest degree of anthropomorphism made cinematic.

---

## ðŸ’­ Reflection Prompts

1. **During the Classroom Turing Game, what made answers feel "human"?** Was it content (what was said), style (how it was said), or imperfection (hesitations, quirks, mistakes)? What does this tell you about what we actually mean by "intelligence"?

2. **You probably say "thank you" to Siri, Alexa, or ChatGPT sometimes. Why?** Is this harmless social habit, or does it subtly reinforce the false belief that the system has feelings? At what point does courtesy become concerning?

3. **Consider the Replika app, which is marketed as an "AI companion."** Users form genuine emotional attachments. If someone finds comfort, support, and connection through a Replika relationship, is that a valid form of companionship â€” or a commercially exploited illusion? What would Searle (Chinese Room) say?

4. **The Boston Dynamics robot videos made many people uncomfortable.** If you felt empathy for the robot, reflect on *why*. Is this response a moral intuition that should be respected â€” or a cognitive error that should be corrected? What would the consequences be of each position?

5. **Amanda Askell's job is to give Claude "a digital soul."** If philosophy can shape AI behaviour in ways that make AI *seem* moral, truthful, and kind â€” but the system doesn't actually *understand* morality, truth, or kindness â€” is this an achievement or a deception? What would Turing say? What would Searle say?

---

*Summary created from: PHKI W2.pdf (16 slides â€” text extraction + image analysis), Week_2Prep.md (resource summaries), SUMMARY_WEEK01.md (cross-references), PHKI FS26 Syllabus, PHKI FS26 Projects brief.*
